---
title: "Relative Value Optimization Project"
author: "Derek Stockman"
output:
  html_document:
    df_print: paged
  word_document: default
--- 

Welcome! With this project, I hope to provide a clear demonstration of my capabilities. I'll utilize both Python and R programming languages. While my experience is primarily with R, I am actively improving my python skills. While I don't claim to be an expert in linear programming nor a developer, I do posess the curiosity and intelligence to deliver decision-useful projects and analysis.

If you are viewing the PDF version of this document, you can visit https://thestockman27.github.io/NS-to-BL-Project/ for the latest version.

**THIS PROJECT IS NOT YET COMPLETE!**

At this time, I am reviewing the output of the optimizations and have noticed some inconsistencies regarding the returns calculated by the Black-Litterman model. The current obstacle is in familiarizing myself with some augmentation to the matrix algebra that the authors of the PortfolioAnalytics package made to the original Black-Litterman formula. When spot-checking the output by hand with the original formula, I arrive at slightly different results. I am no PhD in mathematics, so this issue has slowed progress. That said, let's continue.

#### Outline 
  This project was inspired by my work with factor modeling in corporate bond markets. My research uncovered the persistence of a Value factor in determining excess returns, particularly within the Investment Grade universe. Measurement of Value must account for the term structure of interest rates and spread. It is preferable to perform this analysis upon the Option Adjusted Spread (OAS) of individual bonds, unfortunately I was unable to locate publicly available OAS data, so we will settle for using the Yield-to-Worst (YTW) statistic.
    
  We begin by fitting a curve using the Nelson-Siegel model to the YTW of the bonds included in each sector. From there, we calculate the implied spread to the curve is for each bond. This value represents the potential spread compression possible if the bond were to fully converge with peers. The potential spread compression is multiplied by each bond's duration to arrive at an implied performance. This value will serve as a critical input to the subsequent optimization.
  
  The Black-Litterman model offers a intuitive method for combining an analyst's view with the equilibrium return of an asset. With YTW serving as the equilibrium returns and the implied performance calculated by the previously described relative valuation methodology, we can produce a more intuitive portfolio for capturing the excess return driven by the Value factor.
  
  
#### Set up / Importation / Pre-processing 
```{r}
library(reticulate)
use_python("c:/Users/dstoc/Documents/Python Scripts/First Project/.venv/Scripts/python.exe")
```

We utilize BlackRock iShares Aaa-A Rated Corporate Bond ETF (QLTA) for CUSIP-level details and analytics.
```{python}
import pandas as pd
df = pd.read_excel('C:/Users/dstoc/Documents/Python Scripts/First Project/QLTA.xlsx')
```
```{r, echo = FALSE}
# Load the required libraries
library(DT)

# Create a data table
dt <- py$df

# Render the scrollable table
datatable(dt, options = list(scrollX = TRUE, scrollY = "400px"))
```

```{python, echo = F}
df2 = df[['Name',
 'Sector',
 'Weight (%)',
 'Notional Value',
 'CUSIP',
 #'Price',
 'Duration',
 'YTM (%)',
 'Maturity',
 'Coupon (%)',
 'Mod. Duration',
 'Yield to Worst (%)']]

df2 = df2.iloc[0:len(df2),]

```

#### Analysis
First, convert each bond's maturity to a numeric value for calculation purposes.
```{python Define and Apply MaturityYears, echo=T}
import pandas as pd
from datetime import datetime, timedelta

# Define the date format
date_format = "%b %d, %Y"

# Define a function to convert a date string to the number of days away
def convert_to_years(date_str):
    date_obj = datetime.strptime(date_str, date_format)
    current_date = datetime.now()
    years_away = (date_obj - current_date).days / 365

    if years_away < 0:
        years_away = years_away + 100  # Add 100 years (365.25 days per year on average)
    
    return years_away

# Apply the function to the 'Date' column
df2['MaturityYears'] = df2['Maturity'].apply(convert_to_years)

```
Define a function that fits a Nelson-Siegel curve to the Yield-to-Worst values of individual bonds in each sector.
```{python Define NS Function}
import numpy as np
from nelson_siegel_svensson.calibrate import calibrate_ns_ols, errorfn_ns_ols
from matplotlib import pyplot as plt

def my_NS_func(x_df):
    time = np.array(x_df["MaturityYears"])
    yld = np.array(x_df["Yield to Worst (%)"])

    #plt.plot(time,yld, "r+"); 
    curve, status = calibrate_ns_ols(
        time,yld, tau0=1.0
    )  # starting value of 1.0 for the optimization of tau
    assert status.success
    curve  

    Yhat = x_df['Yield to Worst (%)'].apply(curve)
    x_df.loc[:,'Yhat'] = Yhat

    Spread_to_Curve = x_df['Yield to Worst (%)'] - x_df['Yhat']
    return Spread_to_Curve

```

```{python Apply NS function}
df3 = df2
df3['Spread to Curve'] = df3.groupby('Sector').apply(my_NS_func).reset_index(level=0, drop=True)
```

Below is an example of what happens within the function above, applied to just one sector.
```{python Exemplify NS Curve, echo=F}
df3_sub = df2.loc[df2['Sector'] == 'Communications']
time = np.array(df3_sub["MaturityYears"])
yld = np.array(df3_sub["Yield to Worst (%)"])
curve, status = calibrate_ns_ols(
    time,yld, tau0=0.10
)  # starting value of 1.0 for the optimization of tau
assert status.success
curve
Yhat = df3_sub['MaturityYears'].apply(curve)

```
```{r, echo=FALSE}
# Install and load the necessary packages
library(ggplot2)

# Create a data frame with example data
data <- data.frame(
  x = py$time,
  y = py$yld,
  line = py$Yhat # Series of numbers for the line plot
)

# Create a scatter plot with a line plot
scatter_plot <- ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = line), color = "blue") + # Adding the line plot
  labs(title = "Fiited Nelson-Siegel Curve for the Communications Sector",
       x = "X",
       y = "Y") +
  theme_minimal()

# Print the scatter plot
print(scatter_plot)

```

Calculate additional performance driven by compression of bond's yield to the curve
% Change in Price = Duration x Spread to Curve
```{python}
df3.loc[:,'Implied Performance'] = (df3['Duration'] * df3['Spread to Curve']) + df3['Yield to Worst (%)']
df_final = df3
```
#### Results
```{r, echo = FALSE}
# Load the required libraries
library(DT)

# Create a data table
dt2 <- py$df_final

# Render the scrollable table
datatable(dt2, options = list(scrollX = TRUE, scrollY = "400px"))
```

```{r Reassign output data, echo=F}
NS_output <- py$df_final

```

```{r Define BL Functions, echo=F}
#### define functions ####
BlackLittermanFormula = function( Mu, Sigma, P, v, Omega)
{
  BLMu    = Mu + Sigma %*% t( P ) %*% ( solve( P %*% Sigma %*% t( P ) + Omega ) %*% ( v - P %*% Mu ) );
  BLSigma =  Sigma -  Sigma %*% t( P ) %*% ( solve( P %*% Sigma %*% t( P ) + Omega ) %*% ( P %*% Sigma ) );
  
  return( list( BLMu = BLMu , BLSigma = BLSigma ) );
  
}


black.litterman.corrected <- function (R, P, Mu = NULL, Sigma = NULL, Views = NULL, Omega = NULL) 
{
  if (is.null(Mu)) {
    Mu <- colMeans(R)
  }
  if (length(Mu) != NCOL(R)) 
    stop("length of Mu must equal number of columns of R")
  if (is.null(Sigma)) {
    Sigma <- cov(R)
  }
  if (!all(dim(Sigma) == NCOL(R))) 
    stop("dimensions of Sigma must equal number of columns of R")
  
  if(is.null(Omega)){
  Omega = tcrossprod(P %*% Sigma, P)
  }
  if (is.null(Views)) 
    Views = as.numeric(sqrt(diag(Omega)))
  B = BlackLittermanFormula(Mu, Sigma, P, Views, Omega)
  return(B)
}

portfolio.moments.bl.corrected <- function(R, portfolio, momentargs=NULL, P, Mu=NULL, Sigma=NULL, ...){
  
  
  # If any of the objectives have clean as an argument, we fit the factor
  # model with cleaned returns. Is this the desired behavior we want?
  clean <- unlist(lapply(portfolio$objectives, function(x) x$arguments$clean))
  if(!is.null(clean)){
    if(length(unique(clean)) > 1){
      warning(paste("Multiple methods detected for cleaning returns, default to use clean =", clean[1]))
    }
    # This sets R as the cleaned returns for the rest of the function
    # This is probably fine since the only other place R is used is for the 
    # mu estimate
    R <- Return.clean(R, method=clean[1])
  }
  
  # Compute the Black Litterman estimates
  B <- black.litterman.corrected(R=R, P=P, Mu=Mu, Views = Views , Sigma=Sigma,Omega = Omega)
  
  if(!hasArg(momentargs) | is.null(momentargs)) momentargs<-list()
  if(is.null(portfolio$objectives)) {
    warning("no objectives specified in portfolio")
  } else {
    for (objective in portfolio$objectives){
      switch(objective$name,
             mean = {
               if(is.null(momentargs$mu)) momentargs$mu = B$BLMu
             },
             var =,
             sd =,
             StdDev = { 
               if(is.null(momentargs$mu)) momentargs$mu = B$BLMu
               if(is.null(momentargs$sigma)) momentargs$sigma = B$BLSigma
             },
             mVaR =,
             VaR = ,
             EQS = {
               if(is.null(momentargs$mu)) momentargs$mu = B$BLMu
               if(is.null(momentargs$sigma)) momentargs$sigma = B$BLSigma
               if(is.null(momentargs$m3)) momentargs$m3 = PerformanceAnalytics::M3.MM(R)
               if(is.null(momentargs$m4)) momentargs$m4 = PerformanceAnalytics::M4.MM(R)
             },
             es =,
             mES =,
             CVaR =,
             cVaR =,
             ETL=,
             mETL=,
             ES = {
               # We don't want to calculate these moments if we have an ES 
               # objective and are solving as an LP problem.
               if(hasArg(ROI)) ROI=match.call(expand.dots=TRUE)$ROI else ROI=FALSE
               if(!ROI){
                 if(is.null(momentargs$mu)) momentargs$mu = B$BLMu
                 if(is.null(momentargs$sigma)) momentargs$sigma = B$BLSigma
                 if(is.null(momentargs$m3)) momentargs$m3 = PerformanceAnalytics::M3.MM(R)
                 if(is.null(momentargs$m4)) momentargs$m4 = PerformanceAnalytics::M4.MM(R)
               }
             }
      ) # end switch on objectives    
    }    
  }    
  return(momentargs)
}

#' @title Computes the Black-Litterman formula for the moments of the posterior normal.
#'
#' @description This function computes the Black-Litterman formula for the moments of the posterior normal, as described in  
#' A. Meucci, "Risk and Asset Allocation", Springer, 2005.
#' 
#' @param		Mu       [vector] (N x 1) prior expected values.
#' @param		Sigma    [matrix] (N x N) prior covariance matrix.
#' @param		P        [matrix] (K x N) pick matrix.
#' @param		v        [vector] (K x 1) vector of views.
#' @param		Omega    [matrix] (K x K) matrix of confidence.
#'
#' @return	BLMu     [vector] (N x 1) posterior expected values.
#' @return	BLSigma  [matrix] (N x N) posterior covariance matrix.
#'                              - N = # of assets
#'                              - K = # of Views
#' @references
#' A. Meucci - "Exercises in Advanced Risk and Portfolio Management" \url{http://symmys.com/node/170}.
#'
#' See Meucci's script for "BlackLittermanFormula.m"
#'
#' @author Xavier Valls \email{flamejat@@gmail.com}


```

#### Optimization via the Black-Litterman Model
The Black-Litterman model offers a framework for analysts to update equilibrium returns with their views on specific assets. The model operates like a weighted average between the implied returns and views, with each being scaled by the covariance matrix and the analyst's confidence in each view respectively.

For the purposes of this example, returns were generated in place of observed returns. However, each sector received a unique drift term in an attempt to replicate some level of realistic correlation.

Further, the confidence level of each view is the most difficult parameter to estimate. We allow the model to operate under its default assumption of confidence, that it is equal to the inverse of the covariance matrix. Essentially, the higher the variance in each asset, the less confident we are in the view provided by the Nelson-Siegel model.

```{r, include=FALSE}
library(xts)
```
```{r generate returns, echo=T}
# Create a dataframe populated with the YTW values of each bond.
R <- t(NS_output$`Yield to Worst (%)`)
funds <- NS_output$CUSIP
colnames(R)<- funds
for(i in 1:89){
  R = rbind(R,t(NS_output$`Yield to Worst (%)`))
}

# Generate drift terms for each sector
sector_drifts <- rnorm(length(unique(NS_output$Sector)),0,1)/100

# iterate over each observation for each bond, simulating a random walk with sector-based drift
for(i in 90:1){
  for(k in 1:ncol(R)){
    drift <- sector_drifts[match(NS_output[match(colnames(R)[k],NS_output$CUSIP),"Sector"],
                   unique(NS_output$Sector))]
    if(i==90){
    R[i,k] <- R[i,k] 
    }else{
      R[i,k] <- R[i,k] * (1 + rnorm(1,drift,1)/100)
    }
    }
}
dates <- seq.Date(Sys.Date() - nrow(R)+1 ,Sys.Date(), by = 1)
R <- as.xts(R, order.by = dates)
```
```{r,echo=FALSE}
plot.xts(R[,c(1:3,12,14:19)], main = "Random Walk w/ Drift in Consumer Non-Cyclical", xlab = "Date", ylab = "Yield-to-Worst (%)")
```

We now specify the components of the Black-Litterman Model
```{r set-up BL inputs, echo=T}
Mu <- as.vector(t(NS_output$`Yield to Worst (%)`))
Sigma <- cov(R) 
Views <- NS_output$`Implied Performance`
Picking <- matrix(0, nrow = length(Views), ncol = length(Views))
for(i in 1:nrow(Picking)){
  Picking[i,i] <- 1 # 1 for absolute views
}

Omega <- matrix(0, nrow = length(Views), ncol = length(Views))
diag(Omega) <- runif(length(Views),0,.3) #1
```

We now utilize R's PortfolioAnalytics package to optimize via the Black-Litterman model. We first setup a base portfolio with the fokkowing constraints: 
- it is Long-only
- maximum position size of each bond is 1%
- minimum position size of each bond is equal to the current minimum weight in the portfolio
- the portfolio is fully invested. 

Note that the leverage constraint allows for some leeway when utilizing other optimizers, 

```{r load portfolioanalytics, include=F}
library(PortfolioAnalytics)
```
```{r, First Portfolio, echo=T}
#### First Portfolio ####
bl1.portf <- portfolio.spec(assets=funds)
bl1.portf <- add.constraint(portfolio=bl1.portf, type="full_investment")
bl1.portf <- add.constraint(portfolio=bl1.portf, type="leverage", min_sum=0.99, max_sum=1.01)
bl1.portf <- add.constraint(portfolio=bl1.portf, type="long_only",min_mult =NULL, max_mult=NULL)
bl1.portf <- add.constraint(portfolio = bl1.portf, type = "box",
                            min= min(NS_output$`Weight (%)`)/100,max = .01,min_mult =NULL, max_mult=NULL )
bl1.portf <- add.objective(portfolio=bl1.portf, type="return", name="mean")
bl1.portf


bl1_opt <- optimize.portfolio(R=R, portfolio=bl1.portf, 
                              optimize_method="ROI", trace=TRUE,
                              momentFUN = "portfolio.moments.bl.corrected",
                              P = Picking,
                              Mu = Mu,
                              Sigma = NULL, # allow defaults
                              Views = Views,
                              Omega = NULL  # allow defaults
                              )
print(bl1_opt$objective_measures)

```

In this next portfolio, I add weight constraints to each sector. Allowing the weight of each  to fluctuate by +/- 10% with a lower bound of zero.
```{r Second Portfolio, echo=TRUE}
#### Second Portfolio ####
bl2.portf <- bl1.portf
sector_groups <- list()
group_mins <- list()
group_maxs <- list()
for (i in 1:length(unique(NS_output$Sector))){
  s <- subset(NS_output, Sector == unique(NS_output$Sector)[i])
  for (j in 1:nrow(s)) {
    cusip <- s$CUSIP[j]
    pos <- match(cusip, colnames(R))
    sector_groups[[unique(NS_output$Sector)[i]]][j] <- pos
  }
  group_mins[[i]] <- pmax(sum(s$`Weight (%)`) - 10,0)/100
  group_maxs[[i]] <- pmax(sum(s$`Weight (%)`) + 10,0)/100
}


bl2.portf <- add.constraint(portfolio=bl2.portf, type="group", 
                            groups = sector_groups,
                            group_min = unlist(group_mins),
                            group_max = unlist(group_maxs)
                            )
bl2_opt <- optimize.portfolio(R=R, portfolio=bl2.portf, 
                              optimize_method="ROI", 
                              trace=TRUE,
                              momentFUN = "portfolio.moments.bl.corrected",
                              P = Picking,
                              Mu = Mu,
                              Sigma = NULL,  # allow defaults
                              Views = Views,
                              Omega = NULL   # allow defaults
                              )
print(bl2_opt$objective_measures)
```


Let's explore our results! It appears that there is potential value within the Banking, Electric, and REITs sectors. While this quantitative approach is not all-inclusive, the identification of opportunities within these sectors allows credit analysts to better prioritize their research efforts. Potentially allowing the team to capture even more alpha.
```{r,echo=FALSE}
opt_sector_wts <- extractGroups(bl2_opt)
opt_sector_wts <- opt_sector_wts$group_weights
library(foreach)
init_sector_wts <- foreach(b = unique(NS_output$Sector)) %do%{
  s <- subset(NS_output, Sector == b)
  sum(s$`Weight (%)`)
}
names(init_sector_wts) <- unique(NS_output$Sector)


library(ggplot2)

# Create a data frame with the initial and new weights of assets
weights <- data.frame(
  Sector = unique(NS_output$Sector),
  Initial_Weight = unlist(init_sector_wts)/100,
  New_Weight = opt_sector_wts,row.names = NULL
)
print(weights)
# Reshape the data frame to a long format
weights_long <- reshape2::melt(weights, id.vars = "Sector", variable.name = "Weight_Type", value.name = "Weight")

# Create a bar chart comparing the initial and new weights side by side
chart <- ggplot(weights_long, aes(x = Sector, y = Weight, fill = Weight_Type)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.6) +
  labs(title = "Comparison of Initial Weights to New Weights",
       x = "Sector",
       y = "Weight") +
  scale_fill_manual(values = c("steelblue", "darkorange")) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# Print the chart
print(chart)

```

Next, I intend to add a duration target to stay within a reasonable range from the index's initial value. This should offer an even more practical solution. However, the use of a custom objective function requires the use of an alternative solver (Differential Evolution) that requires significant amount of compute. Therefore, I am simply load the results obtained from one successful optimization.

Other improvements include minimization of another risk factor, such as probability of default.


```{r, Add Duration Target}
#### Third Portfolio ####
# Add duration target
# absolute_duration_deviation <- function(R, weights,holdings,init_port_dur){
#           
#           durations <- holdings$Duration[1:ncol(R)]
#           init_weights <- holdings$`Weight (%)`[1:ncol(R)]
#           weighted_dur <- sum(weights * durations)
#           dur_diff <- abs(weighted_dur - init_port_dur)
#           return(dur_diff)
# }
# 
# bl3.portf <- bl1.portf
# 
# bl3.portf <- add.objective(portfolio=bl3.portf, type="risk", 
#                            name="absolute_duration_deviation",
#                            arguments=list(holdings = NS_output, init_port_dur = 7.17))
# library(DEoptim)
# library(foreach)
# bl3_opt <- optimize.portfolio(R=R, portfolio=bl3.portf, 
#                               optimize_method="DEoptim", 
#                               search_size = 10000000,
#                               itermax = 50,
#                               trace = TRUE,
#                               traceDE = 5,
#                               parallel = TRUE,
#                               max_permutations = 50000,
#                               momentFUN = "portfolio.moments.bl.corrected",
#                               P = Picking,
#                               Mu = Mu,
#                               Sigma = NULL,  # allow defaults
#                               Views = Views,
#                               Omega = NULL   
# )
# 
# print(bl3_opt$objective_measures)
# 
# chart.RiskReward(object = bl3_opt, return.col = "mean", risk.col = "absolute_duration_deviation", main = "Optimization: YTW vs Absolute Duration Deviation")



```

## References
* Black-Litterman
  + https://people.duke.edu/~charvey/Teaching/BA453_2006/Idzorek_onBL.pdf
  + https://github.com/braverock/PortfolioAnalytics
  + https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1314585
* Nelson-Siegel
  + https://www.jstor.org/stable/2352957
  + https://github.com/luphord/nelson_siegel_svensson
* BlackRock
  + https://www.ishares.com/us/products/239431/ishares-aaa-a-rated-corporate-bond-etf
  
